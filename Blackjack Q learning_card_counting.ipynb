{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IhhL4ws7x8K2","executionInfo":{"status":"ok","timestamp":1702324975099,"user_tz":480,"elapsed":20842,"user":{"displayName":"Karthik Jetty","userId":"04257236917899347648"}},"outputId":"30962290-1384-4fe9-e33e-9874a6aa9298"},"id":"IhhL4ws7x8K2","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":3,"id":"d65b602f","metadata":{"id":"d65b602f","executionInfo":{"status":"ok","timestamp":1702324975100,"user_tz":480,"elapsed":9,"user":{"displayName":"Karthik Jetty","userId":"04257236917899347648"}}},"outputs":[],"source":["import enum\n","\n","ranks = {\n","    \"two\" : 2,\n","    \"three\" : 3,\n","    \"four\" : 4,\n","    \"five\" : 5,\n","    \"six\" : 6,\n","    \"seven\" : 7,\n","    \"eight\" : 8,\n","    \"nine\" : 9,\n","    \"ten\" : 10,\n","    \"jack\" : 10,\n","    \"queen\" : 10,\n","    \"king\" : 10,\n","    \"ace\" : (1, 11)\n","}\n","\n","class Suit(enum.Enum):\n","    spades = \"spades\"\n","    clubs = \"clubs\"\n","    diamonds = \"diamonds\"\n","    hearts = \"hearts\"\n","\n","\n","class Card:\n","    def __init__(self, suit, rank, value):\n","        self.suit = suit\n","        self.rank = rank\n","        self.value = value\n","\n","    def __str__(self):\n","        return self.rank + \" of \" + self.suit.value\n","\n","class Deck:\n","    def __init__(self, num=1):\n","        self.cards = []\n","        for i in range(num):\n","            for suit in Suit:\n","                for rank, value in ranks.items():\n","                    self.cards.append(Card(suit, rank, value))\n","\n","    def shuffle(self):\n","        random.shuffle(self.cards)\n","\n","    def deal(self):\n","        return self.cards.pop(0)\n","\n","    def peek(self):\n","        if len(self.cards) > 0:\n","            return self.cards[0]\n","\n","    def add_to_bottom(self, card):\n","        self.cards.append(card)\n","\n","    def __str__(self):\n","        result = \"\"\n","        for card in self.cards:\n","            result += str(card) + \"\\n\"\n","        return result\n","\n","    def __len__(self):\n","        return len(self.cards)"]},{"cell_type":"code","execution_count":4,"id":"65a3affb","metadata":{"id":"65a3affb","executionInfo":{"status":"ok","timestamp":1702324975100,"user_tz":480,"elapsed":8,"user":{"displayName":"Karthik Jetty","userId":"04257236917899347648"}}},"outputs":[],"source":["# This follows the same, official rules every time.\n","# Still need to figure out what happens if there are multiple Aces.\n","def dealer_eval(player_hand):\n","    num_ace = 0\n","    use_one = 0\n","    for card in player_hand:\n","        if card.rank == \"ace\":\n","            num_ace += 1\n","            use_one += card.value[0] # use 1 for Ace\n","        else:\n","            use_one += card.value\n","\n","    if num_ace > 0:\n","        # See if using 11 instead of 1 for the Aces gets the\n","        # dealer's hand value closer to the [17, 21] range\n","\n","        # The dealer will follow Hard 17 rules.\n","        # This means the dealer will not hit again if\n","        # the Ace yields a 17.\n","\n","        # This also means that Aces initially declared as 11's can\n","        # be changed to 1's as new cards come.\n","\n","        ace_counter = 0\n","        while ace_counter < num_ace:\n","            # Only add by 10 b/c 1 is already added before\n","            use_eleven = use_one + 10\n","\n","            if use_eleven > 21:\n","                return use_one\n","            elif use_eleven >= 17 and use_eleven <= 21:\n","                return use_eleven\n","            else:\n","                # The case where even using Ace as eleven is less than 17.\n","                use_one = use_eleven\n","\n","            ace_counter += 1\n","\n","        return use_one\n","    else:\n","        return use_one"]},{"cell_type":"code","execution_count":5,"id":"14c18bd8","metadata":{"id":"14c18bd8","executionInfo":{"status":"ok","timestamp":1702324975100,"user_tz":480,"elapsed":7,"user":{"displayName":"Karthik Jetty","userId":"04257236917899347648"}}},"outputs":[],"source":["def player_eval(player_hand):\n","    num_ace = 0\n","    # use_one means that every ace that in the hand is counted as one.\n","    use_one = 0\n","    for card in player_hand:\n","        if card.rank == \"ace\":\n","            num_ace += 1\n","            use_one += card.value[0] # use 1 for Ace\n","        else:\n","            use_one += card.value\n","\n","    if num_ace > 0:\n","        # Define player policy for Aces:\n","        # Make Aces 11 if they get you to the range [18,21]\n","        # Otherwise, use one.\n","\n","        ace_counter = 0\n","        while ace_counter < num_ace:\n","            # Only add by 10 b/c 1 is already added before\n","            use_eleven = use_one + 10\n","\n","            if use_eleven > 21:\n","                return use_one\n","            elif use_eleven >= 18 and use_eleven <= 21:\n","                return use_eleven\n","            else:\n","                # This allows for some Aces to be 11s, and others to be 1.\n","                use_one = use_eleven\n","\n","            ace_counter += 1\n","\n","        return use_one\n","    else:\n","        return use_one"]},{"cell_type":"code","execution_count":6,"id":"b958cefe","metadata":{"id":"b958cefe","executionInfo":{"status":"ok","timestamp":1702324975100,"user_tz":480,"elapsed":7,"user":{"displayName":"Karthik Jetty","userId":"04257236917899347648"}}},"outputs":[],"source":["def dealer_turn(dealer_hand, deck):\n","    # Calculate dealer hand's value.\n","    dealer_value = dealer_eval(dealer_hand)\n","\n","    # Define dealer policy (is fixed to official rules)\n","\n","    # The dealer keeps hitting until their total is 17 or more\n","    while dealer_value < 17:\n","        # hit\n","        dealer_hand.append(deck.deal())\n","        dealer_value = dealer_eval(dealer_hand)\n","\n","    return dealer_value, dealer_hand, deck"]},{"cell_type":"code","execution_count":7,"id":"e031ecc4","metadata":{"id":"e031ecc4","executionInfo":{"status":"ok","timestamp":1702324976148,"user_tz":480,"elapsed":1054,"user":{"displayName":"Karthik Jetty","userId":"04257236917899347648"}}},"outputs":[],"source":["import random\n","import numpy as np\n","import gym\n","from gym import error, spaces, utils\n","from gym.utils import seeding\n","\n","INITIAL_BALANCE = 1000\n","NUM_DECKS = 2\n","\n","class BlackjackEnv(gym.Env):\n","    metadata = {'render.modes': ['human']}\n","\n","    def __init__(self):\n","        super(BlackjackEnv, self).__init__()\n","\n","        # Initialize the blackjack deck.\n","        self.bj_deck = Deck(NUM_DECKS)\n","\n","        self.player_hand = []\n","        self.dealer_hand = []\n","\n","        self.count = 0\n","\n","        self.reward_options = {\"lose\":-100, \"tie\":0, \"win\":100}\n","\n","        # hit = 0, stand = 1\n","        self.action_space = spaces.Discrete(2)\n","\n","        self.cards_to_add = []\n","\n","        '''\n","        First element of tuple is the range of possible hand values for the player. (3 through 20)\n","        This is the possible range of values that the player will actually have to make a decision for.\n","        Any player hand value 21 or above already has automatic valuations, and needs no input from an\n","        AI Agent.\n","\n","        However, we also need to add all the hand values that the agent could possibly end up in when\n","        they bust. Maybe the agent can glean some correlations based on what hand value they bust at,\n","        so this should be in the observation space. Also, the layout of OpenAI Gym environment class\n","        makes us have to include the bust-value in the step() function because we need to return that\n","        done is true alongside the final obs, which is the bust-value.\n","        '''\n","\n","        # Second element of the tuple is the range of possible values for the dealer's upcard. (2 through 11)\n","        self.observation_space = spaces.Tuple((spaces.Discrete(20), spaces.Discrete(10), spaces.Discrete(13)))\n","\n","        self.done = False\n","\n","    def _take_action(self, action):\n","        if action == 0: # hit\n","            self.player_hand.append(self.bj_deck.deal())\n","\n","        # re-calculate the value of the player's hand after any changes to the hand.\n","        self.player_value = player_eval(self.player_hand)\n","\n","    def step(self, action):\n","        self._take_action(action)\n","\n","        # End the episode/game is the player stands or has a hand value >= 21.\n","        self.done = action == 1 or self.player_value >= 21\n","\n","        # rewards are 0 when the player hits and is still below 21, and they\n","        # keep playing.\n","        rewards = 0\n","\n","        if self.done:\n","            # CALCULATE REWARDS\n","            if self.player_value > 21: # above 21, player loses automatically.\n","                rewards = self.reward_options[\"lose\"]\n","            elif self.player_value == 21: # blackjack! Player wins automatically.\n","                rewards = self.reward_options[\"win\"]\n","            else:\n","                ## Begin dealer turn phase.\n","\n","                dealer_value, self.dealer_hand, self.bj_deck = dealer_turn(self.dealer_hand, self.bj_deck)\n","\n","                ## End of dealer turn phase\n","\n","                #------------------------------------------------------------#\n","\n","                ## Final Compare\n","\n","                if dealer_value > 21: # dealer above 21, player wins automatically\n","                    rewards = self.reward_options[\"win\"]\n","                elif dealer_value == 21: # dealer has blackjack, player loses automatically\n","                    rewards = self.reward_options[\"lose\"]\n","                else: # dealer and player have values less than 21.\n","                    if self.player_value > dealer_value: # player closer to 21, player wins.\n","                        rewards = self.reward_options[\"win\"]\n","                    elif self.player_value < dealer_value: # dealer closer to 21, dealer wins.\n","                        rewards = self.reward_options[\"lose\"]\n","                    else:\n","                        rewards = self.reward_options[\"tie\"]\n","\n","        self.balance += rewards\n","\n","        for card in self.player_hand:\n","            if card.rank == \"ace\":\n","        # Special handling for aces\n","                self.count = self.count - 1\n","            elif card.value > 9:\n","                self.count = self.count - 1\n","            elif card.value < 7:\n","                self.count = self.count + 1\n","\n","        for card in self.dealer_hand:\n","            if card.rank == \"ace\":\n","                # Special handling for aces\n","                self.count = self.count - 1\n","            elif card.value > 9:\n","                self.count = self.count - 1\n","            elif card.value < 7:\n","                self.count = self.count + 1\n","\n","        if(self.count < -5):\n","            self.real_count = -6\n","        elif(self.count > 5):\n","            self.real_count = 6\n","        else:\n","            self.real_count = self.count\n","\n","\n","\n","        # Subtract by 1 to fit into the possible observation range.\n","        # This makes the possible range of 3 through 22 into 1 through 20\n","\n","\n","\n","        count_value_obs = self.real_count + 7\n","\n","        player_value_obs = self.player_value - 2\n","\n","        if self.player_value > 21:\n","          player_value_obs = 20\n","\n","\n","\n","        # get the value of the dealer's upcard, this value is what the agent sees.\n","        # Subtract by 1 to fit the possible observation range of 1 to 10.\n","        upcard_value_obs = dealer_eval([self.dealer_upcard]) - 1\n","\n","        # the state is represented as a player hand-value + dealer upcard pair + what the current count is.\n","        obs = np.array([player_value_obs, upcard_value_obs, count_value_obs])\n","\n","        return obs, rewards, self.done, {}\n","\n","    def reset(self): # resets game to an initial state\n","        # Add the player and dealer cards back into the deck.\n","\n","        self.cards_to_add += self.player_hand + self.dealer_hand\n","        #self.bj_deck.cards += self.player_hand + self.dealer_hand #FIGURE OUT WHAT THIS IS.\n","\n","\n","\n","        # Shuffle before beginning. Only shuffle once before the start of each game. DON\"T DO THIS.\n","        #self.bj_deck.shuffle()\n","\n","        self.balance = INITIAL_BALANCE\n","\n","        self.done = False\n","\n","\n","        # returns the start state for the agent\n","        # deal 2 cards to the agent and the dealer\n","        self.player_hand = [self.bj_deck.deal(), self.bj_deck.deal()]\n","        self.dealer_hand = [self.bj_deck.deal(), self.bj_deck.deal()]\n","        self.dealer_upcard = self.dealer_hand[0]\n","\n","        # calculate the value of the agent's hand\n","        self.player_value = player_eval(self.player_hand)\n","\n","        if self.player_value > 21:\n","          player_value_obs = 21 - 2\n","\n","        #Our count is reinitialized to start at 0.\n","\n","        count_value_obs = self.real_count + 7\n","\n","        #count_value_obs = self.count + 7 DO I NEED TO DO THIS\n","\n","        # Subtract by 1 to fit into the possible observation range.\n","        # This makes the possible range of 2 through 20 into 1 through 18\n","        player_value_obs = self.player_value - 2\n","\n","        # get the value of the dealer's upcard, this value is what the agent sees.\n","        # Subtract by 1 to fit the possible observation range of 1 to 10.\n","        upcard_value_obs = dealer_eval([self.dealer_upcard]) - 1\n","\n","        # the state is represented as a player hand-value + dealer upcard pair.\n","        obs = np.array([player_value_obs, upcard_value_obs, count_value_obs])\n","\n","        return obs\n","\n","    def render(self, mode='human', close=False):\n","        # convert the player hand into a format that is\n","        # easy to read and understand.\n","        hand_list = []\n","        for card in self.player_hand:\n","            hand_list.append(card.rank)\n","\n","        # re-calculate the value of the dealer upcard.\n","        upcard_value = dealer_eval([self.dealer_upcard])\n","\n","        print(f'Balance: {self.balance}')\n","        print(f'Player Hand: {hand_list}')\n","        print(f'Player Value: {self.player_value}')\n","        print(f'Dealer Upcard: {upcard_value}')\n","        print(f'Done: {self.done}')\n","\n","        print()"]},{"cell_type":"code","execution_count":8,"id":"2e3c2306","metadata":{"id":"2e3c2306","colab":{"base_uri":"https://localhost:8080/","height":422},"executionInfo":{"status":"error","timestamp":1702324976149,"user_tz":480,"elapsed":471,"user":{"displayName":"Karthik Jetty","userId":"04257236917899347648"}},"outputId":"414bd4f3-5f93-47ae-c6d3-22bb0606adde"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-40aee1774089>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBlackjackEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0menv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0menv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbj_deck\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-c2ab690877ae>\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;31m#Our count is reinitialized to start at 0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mcount_value_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal_count\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;31m#count_value_obs = self.count + 7 DO I NEED TO DO THIS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'BlackjackEnv' object has no attribute 'real_count'"]}],"source":["env1 = BlackjackEnv()\n","\n","env1.reset()\n","\n","env1.bj_deck.shuffle()\n","\n","\n","total_rewards = 0\n","NUM_EPISODES = 1000\n","\n","for i in range(NUM_EPISODES):\n","    env1.reset()\n","\n","    while env1.done == False:\n","        action = env1.action_space.sample()\n","\n","        new_state, reward, done, desc = env1.step(action)\n","        total_rewards += reward\n","        print(len(env1.dealer_hand))\n","\n","\n","    if (i % 8 == 0):\n","        env1.bj_deck.shuffle()\n","        env1.count = 0\n","        env1.real_count = 0\n","        env1.bj_deck.cards += env1.cards_to_add\n","\n","avg_reward = total_rewards / NUM_EPISODES\n","print(avg_reward)\n","\n","\n","\n","\n","# env1.player_hand = [env1.bj_deck.deal(), env1.bj_deck.deal()]\n","\n","\n","\n","\n","# for card in env1.player_hand:\n","#     print(card.value)\n","#     if((card.value > 9) or (card.rank == \"ace\")):\n","#         env1.count = env1.count - 1\n","#     elif((card.value < 7)):\n","#         env1.count = env1.count + 1\n","# for card in env1.dealer_hand:\n","#     if(card.value > 9 or card.rank == \"ace\"):\n","#         env1.count = env1.count - 1\n","#     elif(card.value < 7):\n","#         env1.count = env1.count + 1\n","\n","# env1.reset()\n","# env1.count = 0\n","# env1.bj_deck.cards += env.cards_to_add\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"903d893f","metadata":{"id":"903d893f","executionInfo":{"status":"aborted","timestamp":1702324976149,"user_tz":480,"elapsed":8,"user":{"displayName":"Karthik Jetty","userId":"04257236917899347648"}}},"outputs":[],"source":["env1.observation_space[2].n\n","\n"]},{"cell_type":"code","execution_count":null,"id":"65d1d626","metadata":{"id":"65d1d626","executionInfo":{"status":"aborted","timestamp":1702324976150,"user_tz":480,"elapsed":9,"user":{"displayName":"Karthik Jetty","userId":"04257236917899347648"}}},"outputs":[],"source":["import random\n","env = BlackjackEnv()\n","\n","total_rewards = 0\n","NUM_EPISODES = 1000\n","\n","for i in range(NUM_EPISODES):\n","    env.reset()\n","\n","    while env.done == False:\n","        action = env.action_space.sample()\n","\n","        new_state, reward, done, desc = env.step(action)\n","        total_rewards += reward\n","\n","\n","    if (i % 10 == 0):\n","        env.bj_deck.shuffle()\n","        env.count = 0\n","        env.real_count = 0\n","        env.bj_deck.cards += env.cards_to_add\n","\n","avg_reward = total_rewards / NUM_EPISODES\n","print(avg_reward)\n"]},{"cell_type":"code","execution_count":null,"id":"4bc46f36","metadata":{"id":"4bc46f36","executionInfo":{"status":"aborted","timestamp":1702324976150,"user_tz":480,"elapsed":9,"user":{"displayName":"Karthik Jetty","userId":"04257236917899347648"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"305a5723","metadata":{"id":"305a5723","executionInfo":{"status":"aborted","timestamp":1702324976150,"user_tz":480,"elapsed":9,"user":{"displayName":"Karthik Jetty","userId":"04257236917899347648"}}},"outputs":[],"source":["def run_mc(env, num_episodes):\n","    '''\n","    observation_space[0] is the 20 possible player values. (3 through 21)\n","    observation_space[1] is the 10 possible dealer upcards. (2 through 11)\n","\n","    Combining these together yields all possible states.\n","\n","    Multiplying this with hit/stand yields all possible state/action pairs.\n","\n","    This is the Q map.\n","    '''\n","    Q = np.zeros([env.observation_space[0].n * env.observation_space[1].n * env.observation_space[2].n, env.action_space.n], dtype=np.float16)\n","\n","\n","    # This map contains the probability distributions for each action (hit or stand) given a state.\n","    # The state (combo of player hand value and dealer upcard value) index in this array yields a 2-element array\n","    # The 0th index of this 2-element array refers to the probability of \"hit\", and the 1st index is the probability of \"stand\"\n","    prob = np.zeros([env.observation_space[0].n * env.observation_space[1].n * env.observation_space[2].n, env.action_space.n], dtype=np.float16) + 0.5\n","\n","    # The learning rate. Very small to avoid making quick, large changes in our policy.\n","    alpha = 0.001\n","\n","    epsilon = 1\n","\n","    # The rate by which epsilon will decay over time.\n","    # Since the probability we take the option with the highest Q-value is 1-epsilon + probability,\n","    # this decay will make sure we are the taking the better option more often in the longrun.\n","    # This allows the algorithm to explore in the early stages, and exploit in the later stages.\n","    decay = 0.9999\n","\n","    # The lowest value that epsilon can go to.\n","    # Although the decay seems slow, it actually grows exponentially, and this is magnified when\n","    # running thousands of episodes.\n","    epsilon_min = 0.9\n","\n","    # may have to be tweaked later.\n","    gamma = 0.8\n","\n","    for i in range(num_episodes):\n","\n","        episode = play_game(env, Q, prob)\n","\n","        epsilon = max(epsilon * decay, epsilon_min)\n","\n","        Q = update_Q(env, episode, Q, alpha, gamma)\n","        prob = update_prob(env, episode, Q, prob, epsilon)\n","\n","        if (i % 10 == 0):\n","            env.bj_deck.shuffle()\n","            env.count = 0\n","            env.bj_deck.cards += env.cards_to_add\n","\n","    return Q, prob"]},{"cell_type":"code","execution_count":null,"id":"ee41bed2","metadata":{"id":"ee41bed2","executionInfo":{"status":"aborted","timestamp":1702324976151,"user_tz":480,"elapsed":8,"user":{"displayName":"Karthik Jetty","userId":"04257236917899347648"}}},"outputs":[],"source":["def play_game(env, Q, prob):\n","    # Can contain numerous state->action->reward tuples because a round of\n","    # Blackjack is not always resolved in one turn.\n","    # However, there will be no state that has a player hand value that exceeds 20, since only initial\n","    # states BEFORE actions are made are used when storing state->action->reward tuples.\n","    episode = []\n","\n","    state = env.reset()\n","\n","\n","    while env.done == False:\n","        if state[0] == 19: #Player was dealt Blackjack, player_value already subtracted by 2 to get state[0]\n","            # don't do any episode analysis for this episode. This is a useless episode.\n","            next_state, reward, env.done, info = env.step(1) # doesn't matter what action is taken.\n","        else:\n","            # Get the index in Q that corresponds to the current state\n","            Q_state_index = get_Q_state_index(state)\n","\n","            # Use the index to get the possible actions, and use np.argmax()\n","            # to get the index of the action that has the highest current Q\n","            # value. Index 0 is hit, index 1 is stand.\n","            best_action = np.argmax(Q[Q_state_index])\n","\n","            # Go to the prob table to retrieve the probability of this action.\n","            # This uses the same Q_state_index used for finding the state index\n","            # of the Q-array.\n","            prob_of_best_action = get_prob_of_best_action(env, state, Q, prob)\n","\n","            action_to_take = None\n","\n","\n","            if random.uniform(0,1) < prob_of_best_action: # Take the best action\n","                action_to_take = best_action\n","            else: # Take the other action\n","                action_to_take = 1 if best_action == 0 else 0\n","\n","            # The agent does the action, and we get the next state, the rewards,\n","            # and whether the game is now done.\n","            next_state, reward, env.done, info = env.step(action_to_take)\n","\n","            # We now have a state->action->reward sequence we can log\n","            # in `episode`\n","            episode.append((state, action_to_take, reward, next_state))\n","\n","            # update the state for the next decision made by the agent.\n","            state = next_state\n","\n","    return episode\n"]},{"cell_type":"code","execution_count":null,"id":"1dff2b25","metadata":{"id":"1dff2b25","executionInfo":{"status":"aborted","timestamp":1702324976152,"user_tz":480,"elapsed":9,"user":{"displayName":"Karthik Jetty","userId":"04257236917899347648"}}},"outputs":[],"source":["def update_Q(env, episode, Q, alpha, gamma):\n","    '''\n","    THIS IS WHERE THE ALGORITHM HINGES ON BEING FIRST VISIT OR EVERY VISIT.\n","    I AM GOING TO USE FIRST-VISIT, AND HERE'S WHY.\n","\n","    If you want first-visit, you need to use the cumulative reward of the entire\n","    episode when updating a Q-value for ALL of the state/action pairs in the\n","    episode, even the first state/action pair. In this algorithm, an episode\n","    is a round of Blackjack. Although the bulk of the reward may come from the\n","    2nd or 3rd decision, deciding to hit on the 1st decision is what enabled\n","    the future situations to even occur, so it is important to include the\n","    entire cumulative reward. We can reduce the impact of the rewards of the\n","    future decisions by lowering gamma, which will lower the G value for our\n","    early state/action pair in which we hit and did not get any immediate rewards.\n","    This will make our agent consider future rewards, and not just look at\n","    each state in isolation despite having hit previously.\n","\n","    If you want Every-Visit MC, do not use the cumulative rewards when updating Q-values,\n","    and just use the immediate reward in this episode for each state/action pair.\n","    '''\n","    # step = 0\n","\n","    # for state, action, reward in episode:\n","    #   next_max = max(Q[next_state].values())\n","    #   # Calculate the new Q-value\n","    #   Q_state_index = get_Q_state_index(state)\n","    #   curr_Q_value = Q[Q_state_index][action]\n","    #   Q[Q_state_index][action] = curr_Q_value + alpha * (reward + gamma * next_max - curr_Q_value)\n","\n","    # step += 1\n","\n","    # return Q\n","\n","\n","\n","\n","    step = 0\n","\n","    for state, action, reward, next_state in episode:\n","        Q_state_index = get_Q_state_index(state)\n","        next_state_index = get_Q_state_index(next_state)\n","        next_max = np.max(Q[next_state_index])\n","        curr_Q_value = Q[Q_state_index][action]\n","        Q[Q_state_index][action] = curr_Q_value + alpha * (reward + gamma * next_max - curr_Q_value)\n","    step += 1\n","\n","    return Q\n","\n","\n","\n","\n","    # for state, action, reward in episode:\n","    #     # calculate the cumulative reward of taking this action in this state.\n","    #     # Start from the immediate rewards, and use all the rewards from the\n","    #     # subsequent states. Do not use rewards from previous states.\n","    #     total_reward = 0\n","    #     gamma_exp = 0\n","    #     for curr_step in range(step, len(episode)):\n","    #         curr_reward = episode[curr_step][2]\n","    #         total_reward += (gamma ** gamma_exp) * curr_reward\n","    #         gamma_exp += 1\n","\n","    #     # Update the Q-value\n","    #     Q_state_index = get_Q_state_index(state)\n","    #     curr_Q_value = Q[Q_state_index][action]\n","    #     Q[Q_state_index][action] = curr_Q_value + alpha * (total_reward - curr_Q_value)\n","\n","    #     # update step to start further down the episode next time.\n","    #     step += 1\n","\n","\n","    # return Q"]},{"cell_type":"code","execution_count":null,"id":"3e81316c","metadata":{"id":"3e81316c","executionInfo":{"status":"aborted","timestamp":1702324976152,"user_tz":480,"elapsed":9,"user":{"displayName":"Karthik Jetty","userId":"04257236917899347648"}}},"outputs":[],"source":["\n","def update_prob(env, episode, Q, prob, epsilon):\n","    for state, action, reward, next_state in episode:\n","        # Update the probabilities of the actions that can be taken given the current\n","        # state. The goal is that the new update in Q has changed what the best action\n","        # is, and epsilon will be used to create a small increase in the probability\n","        # that the new, better action is chosen.\n","        prob = update_prob_of_best_action(env, state, Q, prob, epsilon)\n","\n","    return prob"]},{"cell_type":"code","execution_count":null,"id":"40cc5a9d","metadata":{"id":"40cc5a9d","executionInfo":{"status":"aborted","timestamp":1702324976152,"user_tz":480,"elapsed":9,"user":{"displayName":"Karthik Jetty","userId":"04257236917899347648"}}},"outputs":[],"source":["# Given a state, derive the corresponding index in the Q-array.\n","# The state is a player hand value + dealer upcard pair,\n","# so a \"hashing\" formula must be used to allocate the\n","# indices of the Q-array properly.\n","def get_Q_state_index(state):\n","    # the player value is already subtracted by 1 in the env when it returns the state.\n","    # subtract by 1 again to fit with the array indexing that starts at 0\n","    initial_player_value = state[0] - 1\n","    # the upcard value is already subtracted by 1 in the env when it returns the state.\n","    # dealer_upcard will be subtracted by 1 to fit with the array indexing that starts at 0\n","    dealer_upcard = state[1] - 1\n","\n","    count_of_deck = state[2] - 1\n","\n","    index = (initial_player_value * env.observation_space[1].n * env.observation_space[2].n) + (dealer_upcard * env.observation_space[2].n) + count_of_deck\n","\n","    #index = (value1 * size_range2 * size_range3) + (value2 * size_range3) + value3\n","\n","    # Calculate the index\n","    # The index is a combination of the player's value, dealer's upcard, and the normalized count\n","    #index = (total_player_and_dealer_states * count_of_deck) + (env.observation_space[1].n * initial_player_value) + dealer_upcard\n","\n","    return index"]},{"cell_type":"code","execution_count":null,"id":"1eec7310","metadata":{"id":"1eec7310","executionInfo":{"status":"aborted","timestamp":1702324976152,"user_tz":480,"elapsed":9,"user":{"displayName":"Karthik Jetty","userId":"04257236917899347648"}}},"outputs":[],"source":["def get_prob_of_best_action(env, state, Q, prob):\n","    # Use the mapping function to figure out which index of Q corresponds to\n","    # the player hand value + dealer upcard value that defines each state.\n","    Q_state_index = get_Q_state_index(state)\n","\n","    # Use this index in the Q 2-D array to get a 2-element array that yield\n","    # the current Q-values for hitting (index 0) and standing (index 1) in this state.\n","    # Use the np.argmax() function to find the index of the action that yields the\n","    # rewards i.e. the best action we are looking for.\n","    best_action = np.argmax(Q[Q_state_index])\n","\n","    # Retrieve the probability of the best action using the\n","    # state/action pair as indices for the `prob` array,\n","    # which stores the probability of taking an action (hit or stand)\n","    # for a given state/action pair.\n","    return prob[Q_state_index][best_action]\n","\n","def update_prob_of_best_action(env, state, Q, prob, epsilon):\n","\n","    Q_state_index = get_Q_state_index(state)\n","\n","    best_action = np.argmax(Q[Q_state_index])\n","\n","    # Slightly alter the probability of this best action being taken by using epsilon\n","    # Epsilon starts at 1.0, and slowly decays over time.\n","    # Therefore, as per the equation below, the AI agent will use the probability listed\n","    # for the best action in the `prob` array during the beginning of the algorithm.\n","    # As time goes on, the likelihood that the best action is taken is increased from\n","    # what is listed in the `prob` array.\n","    # This allows for exploration of other moves in the beginning of the algorithm,\n","    # but exploitation later for a greater reward.\n","    #prob[Q_state_index][best_action] = prob[Q_state_index][] + ((1 - epsilon) * (1 - prob[Q_state_index][best_action]))\n","    prob[Q_state_index][best_action] = min(1, prob[Q_state_index][best_action] + 1 - epsilon)\n","\n","    other_action = 1 if best_action == 0 else 0\n","    prob[Q_state_index][other_action] = 1 - prob[Q_state_index][best_action]\n","\n","    return prob"]},{"cell_type":"code","execution_count":1,"id":"045d53ff","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":246},"id":"045d53ff","executionInfo":{"status":"error","timestamp":1702324953787,"user_tz":480,"elapsed":303,"user":{"displayName":"Karthik Jetty","userId":"04257236917899347648"}},"outputId":"958a95f2-b0fa-48dc-8001-ad1119d5d63d"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-40a52cd05745>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBlackjackEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_times\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'BlackjackEnv' is not defined"]}],"source":["import time\n","env = BlackjackEnv()\n","\n","num_times = 1000\n","\n","start_time = time.time()\n","new_Q, new_prob = run_mc(env, num_times)\n","end_time = time.time()\n","\n","print(\"Total Time for Learning: \" + str(end_time - start_time))"]},{"cell_type":"code","execution_count":null,"id":"b4e3f47d","metadata":{"id":"b4e3f47d","executionInfo":{"status":"aborted","timestamp":1702301744753,"user_tz":480,"elapsed":23,"user":{"displayName":"Karthik Jetty","userId":"04257236917899347648"}}},"outputs":[],"source":["def best_policy(Q):\n","    best_policy_binary = []\n","    best_policy_string = []\n","    best_policy_colors = []\n","    for i in range(len(Q)):\n","        best_policy_binary.append(np.argmax(Q[i]))\n","        best_policy_string.append(\"Hit\" if np.argmax(Q[i]) == 0 else \"Stand\")\n","        best_policy_colors.append(\"y\" if np.argmax(Q[i]) == 0 else \"m\")\n","\n","    return best_policy_binary, best_policy_string, best_policy_colors"]},{"cell_type":"code","source":[],"metadata":{"id":"hwmOjGOsx6zl","executionInfo":{"status":"aborted","timestamp":1702301744753,"user_tz":480,"elapsed":23,"user":{"displayName":"Karthik Jetty","userId":"04257236917899347648"}}},"id":"hwmOjGOsx6zl","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"d004b8c9","metadata":{"id":"d004b8c9","executionInfo":{"status":"aborted","timestamp":1702301744753,"user_tz":480,"elapsed":23,"user":{"displayName":"Karthik Jetty","userId":"04257236917899347648"}}},"outputs":[],"source":["import pandas as pd\n","new_Q_binary, new_Q_string, new_Q_colors = best_policy(new_Q)\n","\n","df = pd.DataFrame(columns = range(-6, 7))\n","\n","color_df = pd.DataFrame(columns = range(-6, 7))\n","\n","print(df)\n","\n","print(new_Q_string[2417:2430])\n","\n","def get_indices_for_value1(value1, value2_fixed):\n","    size_range3 = 13  # Size of the third range (0-12)\n","    indices = []\n","\n","    arr = []\n","\n","    for value3 in range(size_range3):\n","\n","        arr=[value1, value2_fixed, value3]\n","        index = get_Q_state_index(arr)\n","        indices.append(index)\n","        arrr = []\n","\n","    return indices\n","\n","\n","print(get_indices_for_value1(6, 1))\n","\n","# Example usage\n","value1 = 21  # Within the range 0-17\n","value2_fixed = 0  # A fixed value for range2 (0-9)\n","\n","indices_list = get_indices_for_value1(value1, value2_fixed)\n","print(f\"Indices for value1 = {value1} with value2 fixed at {value2_fixed}: {indices_list}\")\n","\n","\n","\n","for value in range(2, 11):\n","    for s in range(3, 23): # possible player values in the range 3 to 20\n","\n","        indices_list = get_indices_for_value1(s-2, value)  # Adjusted for zero-based indexing\n","        start = indices_list[0]\n","        end = start + 13  # +1 because slice end index is exclusive\n","\n","        df.loc[s] = new_Q_string[start:end]\n","        color_df.loc[s] = new_Q_colors[start:end]\n","\n","\n","    df.loc[s]=(new_Q_string[start:end])\n","    color_df.loc[s]=(new_Q_colors[start:end])\n","    fig, ax = plt.subplots()\n","\n","\n","    ax.set_axis_off()\n","\n","\n","#ax.axis('tight')\n","\n","    ax.table(cellText=df.values, cellColours=color_df.values, rowLabels=df.index, colLabels=df.columns, loc='center')\n","\n","    fig.tight_layout()\n","\n","    s1 = \"Player Value vs. Count for Dealer Upcard value: \"\n","\n","    var = s1 + str(value)\n","\n","\n","    plt.title(var)\n","    plt.xlabel(\"Average Pulse\")\n","    plt.ylabel(\"Calorie Burnage\")\n","\n","    plt.show()\n","\n","\n","    fig.savefig('/content/drive/MyDrive/CS_238/' + str(value) + '_Dealer_card_value.png', bbox_inches='tight', dpi = 1000)\n"]},{"cell_type":"code","execution_count":null,"id":"d33dd0a1","metadata":{"id":"d33dd0a1","executionInfo":{"status":"aborted","timestamp":1702301744753,"user_tz":480,"elapsed":23,"user":{"displayName":"Karthik Jetty","userId":"04257236917899347648"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","fig, ax = plt.subplots()\n","\n","# hide axes\n","#fig.patch.set_visible(False)\n","ax.set_axis_off()\n","#ax.axis('tight')\n","\n","ax.table(cellText=df.values, cellColours=color_df.values, rowLabels=df.index, colLabels=df.columns, loc='center')\n","\n","fig.tight_layout()\n","\n","\n","#plt.title(\"Blackjack actions for %d iterations and count\" (num_times))\n","plt.xlabel(\"Average Pulse\")\n","plt.ylabel(\"Calorie Burnage\")\n","\n","plt.show()"]},{"cell_type":"code","execution_count":9,"id":"b6c0a116","metadata":{"id":"b6c0a116","colab":{"base_uri":"https://localhost:8080/","height":367},"executionInfo":{"status":"error","timestamp":1702303215804,"user_tz":480,"elapsed":383,"user":{"displayName":"Karthik Jetty","userId":"04257236917899347648"}},"outputId":"afec716d-af03-43a5-8afa-5c2acb32da20"},"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-a4a82231e9b8>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPISODES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-c2ab690877ae>\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;31m#Our count is reinitialized to start at 0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mcount_value_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal_count\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;31m#count_value_obs = self.count + 7 DO I NEED TO DO THIS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'BlackjackEnv' object has no attribute 'real_count'"]}],"source":["import random\n","env = BlackjackEnv()\n","\n","total_rewards = 0\n","NUM_EPISODES = 2000\n","\n","reward_arr = [0] * NUM_EPISODES\n","\n","\n","\n","\n","for i in range(NUM_EPISODES):\n","    state = env.reset()\n","\n","    while env.done == False:\n","\n","\n","\n","        if (i % 10 == 0):\n","            env.bj_deck.shuffle()\n","            env.count = 0\n","            env.real_count = 0\n","            env.bj_deck.cards += env.cards_to_add\n","\n","\n","        if state[0] == 19: # Player was dealt Blackjack\n","            next_state, reward, env.done, info = env.step(1) # doesn't matter what action is taken.\n","            # don't do any th analysis for this episode. This is a useless episode.\n","            total_rewards += reward\n","\n","\n","        else:\n","            Q_index = get_Q_state_index(state)\n","            action = new_Q_binary[Q_index]\n","\n","            new_state, reward, done, desc = env.step(action)\n","            state = new_state\n","            total_rewards += reward\n","\n","        reward_arr[i] = (total_rewards)/(i+1)\n","\n","\n","x_axis = np.arange(1, 2001)\n","\n","# data to be plotted\n","\n","\n","fig, ax = plt.subplots()\n","\n","# Plotting\n","ax.set_title(\"Blackjack Card Counting\")\n","ax.set_xlabel(\"Iterations\")\n","ax.set_ylabel(\"Average Reward\")\n","ax.plot(x_axis, reward_arr, color=\"red\")\n","\n","# Display the plot\n","plt.show()\n","\n","# Save the figure\n","fig.savefig('/content/drive/MyDrive/CS_238/Blackjack_Card_Counting.png', bbox_inches='tight', dpi=1000)\n","\n","avg_reward = total_rewards / NUM_EPISODES\n","print(avg_reward)\n","\n","\n","\n","\n","\n","\n","#make a graph for the rewards over time for number of simulations\n","\n","#make a graph for each of the different types of simulations\n","\n","#make plots for different amount of runtimes (show the runtime given 100000000 simulations for non-cheating) then do the same thing for cheating but do lower number of simulations\n","\n","#make plots for the scenarios for different counts\n","\n","#Do the Q-learning algorithm\n","\n","#"]},{"cell_type":"code","execution_count":null,"id":"0692846c","metadata":{"id":"0692846c","executionInfo":{"status":"aborted","timestamp":1702301744754,"user_tz":480,"elapsed":24,"user":{"displayName":"Karthik Jetty","userId":"04257236917899347648"}}},"outputs":[],"source":["import random\n","env = BlackjackEnv()\n","\n","total_rewards = 0\n","NUM_EPISODES = 10000\n","\n","for i in range(NUM_EPISODES):\n","    env.reset()\n","\n","    while env.done == False:\n","        action = env.action_space.sample()\n","\n","        new_state, reward, done, desc = env.step(action)\n","        total_rewards += reward\n","\n","\n","\n","    if (i % 10 == 0):\n","        env.bj_deck.shuffle()\n","        env.count = 0\n","        env.real_count = 0\n","        env.bj_deck.cards += env.cards_to_add\n","\n","avg_reward = total_rewards / NUM_EPISODES\n","print(avg_reward)\n"]},{"cell_type":"code","execution_count":null,"id":"ce6c0ff3","metadata":{"id":"ce6c0ff3","executionInfo":{"status":"aborted","timestamp":1702301744754,"user_tz":480,"elapsed":24,"user":{"displayName":"Karthik Jetty","userId":"04257236917899347648"}}},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}